<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extensive Reading During FIS Research</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #f4f4f9;
            color: #333;
            margin: 0;
            padding: 20px;
        }

        #sidebar {
            position: fixed;
            top: 20%;
            right: 20px;
            width: 10%;
            margin-left: 0;
            padding: 15px;
            background-color: #f9f9f9ff;
            border: 1px solid #ddd;
            border-radius: 5px;
        }

        #sidebar p {
            font: 1.3em sans-serif;
        }

        #sidebar nav ul {
            list-style-type: none;
            padding: 0;
        }

        #sidebar nav ul li {
            margin-bottom: 10px;
        }

        #sidebar nav ul li a {
            text-decoration: none;
            color: #007BFF;
            transition: color 0.3s ease;
        }

        #sidebar nav ul li a:hover {
            color: #0056b3;
        }

        header {
            text-align: center;
            margin-bottom: 20px;
        }

        h1 {
            font-size: 2.5em;
        }

        p {
            color: #555;
        }

        fieldset {
            border: 1px solid #ddd;
            border-radius: 5px;
            margin-bottom: 20px;
            padding: 15px;
            background-color: #fff;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        legend {
            font-weight: bold;
            color: #555;
            padding: 0 5px;
            font-size: 1.7em;
        }

        ul {
            list-style-type: none;
            padding: 0;
        }

        li {
            margin-bottom: 10px;
        }

        a {
            text-decoration: none;
            color: #007BFF;
            transition: color 0.3s ease;
        }

        a:hover {
            color: #0056b3;
        }
    </style>
</head>
<body>
    <header>
        <h1>Some Extensive Reading on the Internet During FIS Research</h1>
        <p>Read a lot of knowledge in various directions and do a lot of experiments inspired by them, only a few help, but thank them too.</p>
    </header>
    <aside id="sidebar">
        <nav aria-labelledby="sidebarTitle">
            <p id="sidebarTitle">Content</p>
            <ul role="list">
                <li><a href="#convolution">Convolution</a></li>
                <li><a href="#attention">Attention</a></li>
                <li><a href="#basic">Basic</a></li>
                <li><a href="#diffusionmodel">DiffusionModel</a></li>
                <li><a href="#inn">INN</a></li>
                <li><a href="#mtl">MTL</a></li>
                <li><a href="#loss">Loss</a></li>
                <li><a href="#frequency">Frequency</a></li>
                <li><a href="#opticflow">OpticFlow</a></li>
                <li><a href="#upsampling">UpSampling</a></li>
                <li><a href="#metalearning">Meta Learning</a></li>
                <li><a href="#others">Others</a></li>
            </ul>
        </nav>
    </aside>
    <main>
        <fieldset id="convolution">
            <legend>Convolution</legend>
            <ul>
                <li><a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/40050371">1x1卷积核</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/50369448">空洞卷积(Dilated/Atrous Convolution)</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/384215504">转置卷积、空洞卷积和可变形卷积</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/226448051">分组卷积(Group conv)与深度可分离卷积(Depthwise separable conv)</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/381839221">20种卷积</a></li>
                <li><a href="https://blog.csdn.net/gbz3300255/article/details/105769822">Depthwise Separable Convolution原理</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/452806995">[论文]Large Kernel Matters</a></li>
                <li><a href="https://github.com/ogvalt/large_kernel_matters/blob/master/scripts/model.py">large_kernel_matters/scripts/model.py</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/93966695">ICCV2019-3*3卷积+1*3卷积+3*1卷积</a></li>
                <li><a href="https://github.com/DingXiaoH/ACNet">ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks</a></li>
                <li><a href="https://blog.csdn.net/weixin_47196664/article/details/115388486">CVPR2021-Diverse branch block</a></li>
                <li><a href="https://blog.csdn.net/PLANTTHESON/article/details/134166792">ICLR 2022-ODConv</a></li>
                <li><a href="https://blog.csdn.net/weixin_44402973/article/details/115137571">动态卷积Dynamic Convolution</a></li>
            </ul>
        </fieldset>
        
        <fieldset id="attention">
            <legend>Attention</legend>
            <ul>
                <li><a href="https://zhuanlan.zhihu.com/p/83665899?from=groupmessage">CBAM: Convolutional Block Attention Module</a></li>
                <li><a href="https://mp.weixin.qq.com/s/Nn5sLt_0MIaXNYxKv0N1Hg">Triplet注意力机制</a></li>
                <li><a href="https://blog.csdn.net/Evan123mg/article/details/80058077">SENet</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/80123284">SENet</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/443474114">A2-Nets: Double Attention Networks</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/110668476">Non-local-Block以及其他注意力机制</a></li>
                <li><a href="https://cloud.tencent.com/developer/article/1925513">LVT | ViT轻量化</a></li>
            </ul>
        </fieldset>
        
        <fieldset id="basic">
            <legend>Basic</legend>
            <ul>
                <li><a href="https://zhuanlan.zhihu.com/p/433836153">深度学习中的超参数调节（learning rate、epochs、batch-size...） - 知乎</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/90020601">【AutoML】优化方法可以进行自动搜索学习吗？ - 知乎</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/172254089">一文搞懂激活函数(Sigmoid/ReLU/LeakyReLU/PReLU/ELU) - 知乎</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/261059231">损失函数（Loss Function） - 知乎</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/142866736">深度学习中的标准化—Normalization Methods in Deep Learning</a></li>
                <li><a href="https://blog.csdn.net/weixin_39228381/article/details/108310520">pytorch优化器详解</a></li>
                <li><a href="https://blog.csdn.net/pipisorry/article/details/71157037">激活函数glu</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/150181052">Group Normalization - 知乎</a></li>
            </ul>
        </fieldset>
        
        <fieldset id="diffusionmodel">
            <legend>DiffusionModel</legend>
            <ul>
                <li><a href="https://zhuanlan.zhihu.com/p/638442430">Diffusion Model详解：直观理解、数学原理、PyTorch 实现</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/563543020">通俗理解扩散模型</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/627616358">DDIM凭什么加速DDPM的采样效率</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/666015077">DDIM反转和音频扩散</a></li>
            </ul>
        </fieldset>
        
        <fieldset id="inn">
            <legend>INN</legend>
            <ul>
                <li><a href="https://zhuanlan.zhihu.com/p/268242678">神经网络的可逆形式 - 知乎</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/419459704">可逆神经网络(Invertible Neural Networks, INN)详细解析</a></li>
            </ul>
        </fieldset>

        <fieldset id="mtl">
            <legend>MTL</legend>
            <ul>
                <li><a href="https://www.zhihu.com/question/375794498">深度学习的多个loss如何平衡?</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/367881339">多任务权重自动学习论文介绍和代码实现</a></li>
                <li><a href="https://github.com/Mikoto10032/AutomaticWeightedLoss/tree/master">Mikoto10032/AutomaticWeightedLoss: Multi-task learning using uncertainty to weigh losses for scene geometry and semantics, Auxiliary Tasks in Multi-task Learning</a></li>
                <li><a href="https://github.com/AidasLiaudanskas/AutomaticLossWeightingPyTorch/blob/master/weighted_loss.py">AutomaticLossWeightingPyTorch/weighted_loss.py at master · AidasLiaudanskas/AutomaticLossWeightingPyTorch</a></li>
                <li><a href="https://www.zhihu.com/question/359962155/answers/updated">多任务学习中各loss权重应该如何设计</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/598657351">【多任务】任务损失/梯度优化策略合集</a></li>
                <li><a href="https://github.com/abcdump/Pytorch-PCGrad-GradVac-AMP-GradAccum/tree/master">abcdump/Pytorch-PCGrad-GradVac-AMP-GradAccum: PyTorch 1.11 reimplementation of multi task gradient adaptation ideas: Gradient Surgery (PCGrad) and Gradient Vaccine</a></li>
            </ul>
        </fieldset>
        
        <fieldset id="loss">
            <legend>Loss</legend>
            <ul>
                <li><a href="https://github.com/xxgege/GAM">xxgege/GAM: The official repo for CVPR2023 highlight paper "Gradient Norm Aware Minimization Seeks First-Order Flatness and Improves Generalization".</a></li>
                <li><a href="https://blog.csdn.net/qq_43414059/article/details/118916427">不可导AP-Loss</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/260703521">A General Adaptive Robust Loss Function - 知乎</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/359355486">自适应隐写算法HiLL - 知乎</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/80187137">边缘引导loss在显著性检测中的应用</a></li>
                <li><a href="https://blog.csdn.net/yexiaogu1104/article/details/88395475">TV Loss</a></li>
            </ul>
        </fieldset>
        
        <fieldset id="frequency">
            <legend>Frequency</legend>
            <ul>
                <li><a href="https://zhuanlan.zhihu.com/p/494060193">论文汇总|如何从频域视角提升图像生成质量 (Frequency GANs)</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/342991714">频域(DCT,小波变换)与CNN结合</a></li>
                <li><a href="https://github.com/dcdcvgroup/FcaNet/blob/master/model/layer.py#L29">FcaNet/model/layer.py at master · dcdcvgroup/FcaNet</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/315601295">CNN</a></li>
            </ul>
        </fieldset>

        <fieldset id="opticflow">
            <legend>OpticFlow</legend>
            <ul>
                <li><a href="https://zhuanlan.zhihu.com/p/532681825">CVPR 2022 Oral | 基于全局匹配的光流学习框架</a></li>
                <li><a href="https://github.com/zacjiang/GMA/tree/main/core">GMA/core at main · zacjiang/GMA</a></li>
                <li><a href="https://github.com/mulns/AccFlow">mulns/AccFlow: Official code for paper "AccFlow: Backward Accumulation for Long-Range Optical Flow"</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/668312009">2023 optical flow整理</a></li>
                <li><a href="https://github.com/open-mmlab/mmflow">open-mmlab/mmflow: OpenMMLab optical flow toolbox and benchmark</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/484698323">RAFT算法 ECCV2020最佳论文</a></li>
                <li><a href="https://blog.csdn.net/qq_43414059/article/details/108842025">RAFT: Recurrent All-Pairs Field Transforms for Optical Flow_CyrilSterling</a></li>
                <li><a href="https://github.com/JiaRenChang/PSMNet">JiaRenChang/PSMNet: Pyramid Stereo Matching Network (CVPR2018)</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/584007455">ECCV 2022-FlowFormer 光流估计新范式</a></li>
            </ul>
        </fieldset>
        
        <fieldset id="upsampling">
            <legend>UpSampling</legend>
            <ul>
                <li><a href="https://zhuanlan.zhihu.com/p/48501100">反卷积(Transposed Convolution)详细推导</a></li>
                <li><a href="https://blog.csdn.net/u011974639/article/details/79460893">语义分割--Understand Convolution for Semantic Segmentation_understanding convolution for semantic segmentatio</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/62508574">CVPR2019-DUpsampling数据相关型解码方式</a></li>
                <li><a href="https://github.com/haochange/DUpsampling/blob/master/models/dunet.py">DUpsampling/models/dunet.py at master · haochange/DUpsampling</a></li>
            </ul>
        </fieldset>
        
        <fieldset id="metalearning">
            <legend>Meta Learning</legend>
            <ul>
                <li><a href="https://blog.csdn.net/weixin_42392454/article/details/127228377">Meta-SGD原理讲解和代码实现</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/89505918">三张图理解「优化优化器的优化器」加上REINFORCE算法迭代求解 - 知乎</a></li>
                <li><a href="https://paperswithcode.com/paper/learning-to-learn-by-gradient-descent-by">Learning to learn by gradient descent by gradient descent | Papers With Code</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/28639662">Meta Learning/Learning to learn - 知乎</a></li>
                <li><a href="https://github.com/learnables/learn2learn">learnables/learn2learn: A PyTorch Library for Meta-learning Research</a></li>
                <li><a href="https://github.com/odlgroup/odl">odlgroup/odl: Operator Discretization Library https://odlgroup.github.io/odl/</a></li>
                <li><a href="https://blog.csdn.net/MR_kdcon/article/details/120358472">Meta-RL之Learning to Learn by gd by gd</a></li>
                <li><a href="https://blog.csdn.net/senius/article/details/84483329">Learning to learn by gradient descent by gradient descent - PyTorch实践</a></li>
                <li><a href="https://blog.csdn.net/MR_kdcon/article/details/120844717">Meta-Learning之Meta-SGD</a></li>
            </ul>
        </fieldset>

        <fieldset id="others">
            <legend>Others</legend>
            <ul>
                <li><a href="https://aijishu.com/a/1060000000258839">Self-Attention与CNN融合范式</a></li>
                <li><a href="https://zhuanlan.zhihu.com/p/357347508">Self-Attention ConvLSTM for Spatiotemporal Prediction</a></li>
                <li><a href="https://www.zhihu.com/column/c_1208033701705162752">时空序列预测</a></li>
            </ul>
        </fieldset>
    </main>
</body>
</html>